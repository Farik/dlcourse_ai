{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 06 - RNNs, part 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJW9PnaqE5n_",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a927a293-0a32-4e5c-b0cc-237455d890d6"
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5MB 31kB/s \n",
            "\u001b[31mERROR: torchvision 0.3.0 has requirement torch>=1.1.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.54 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0MB 6.3MB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 6.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ddd46e08-707a-4cc0-993f-723dbf89ae16"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "be5a63ba-b3fe-4ee3-8e9d-83c444167ebb"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "47a6291b-2166-4fd7-bffa-96aed3100554"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "33723fb1-533a-463c-8ce5-a2521d8a7256"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'PRON', 'NOUN', 'VERB', 'ADJ', 'ADP', 'CONJ', 'X', 'ADV', '.', 'NUM', 'PRT', 'DET'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "7793ad45-b27a-4ad3-8955-d02a69c8ff5e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAEyCAYAAABH+Yw/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c35a395-55da-4e55-ec4e-bbb7d0d66620"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "710f7354-41d3-40ae-9315-b6005c5f0c1a"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80efd2b7-d469-479d-ad62-de4cb633b51e"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2202c674-729f-478f-9973-deb41973ed5f"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = lstm_hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim, self.hidden_dim)\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        self.hidden2tag = nn.Linear(self.hidden_dim*4, tagset_size)\n",
        "\n",
        "#     def init_hidden(self):\n",
        "#         hidden_a = torch.randn(self.hparams.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        print(\"input\",inputs.shape)\n",
        "        \n",
        "      \n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        print(\"embeds\",embeds.shape)\n",
        "        \n",
        "        pad_sizes = torch.full((inputs.shape[0],),inputs.shape[1],dtype=torch.int64)\n",
        "        \n",
        "        \n",
        "        # pack the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeds, pad_sizes, batch_first=True)\n",
        "        \n",
        "        packed_output, _ = self.lstm(packed)\n",
        "#         print(packed_output.shape)\n",
        "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        print(\"packed lstm\",lstm_out.shape)\n",
        "        lstm_out = lstm_out.contiguous()\n",
        "        lstm_out = lstm_out.view(inputs.shape[0], -1)\n",
        "        print(lstm_out.shape)\n",
        "        \n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        #tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "8b04d268-042b-4ff8-a8ce-0d6f0cd1710c"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input torch.Size([32, 4])\n",
            "embeds torch.Size([32, 4, 100])\n",
            "packed lstm torch.Size([32, 4, 128])\n",
            "torch.Size([32, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "<calc loss>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = <calc loss>\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = <calc accuracy>\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "colab": {}
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVSpedXdE5o_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {}
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        <create me>\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <use me>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {}
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {}
      },
      "source": [
        "<calc test accuracy>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}